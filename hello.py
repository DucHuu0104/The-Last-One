import streamlit as st
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_community.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.prompts import PromptTemplate
from docx import Document
from dotenv import load_dotenv
from pptx import Presentation
import tempfile
import os
import google.generativeai as genai
from io import BytesIO
import matplotlib.pyplot as plt
from collections import Counter
import seaborn as sns
from wordcloud import  STOPWORDS

from PIL import Image
import pytesseract
# Load biáº¿n mÃ´i trÆ°á»ng
load_dotenv()
api_key = os.getenv('GEMINI_API_KEY')

if not api_key:
    st.error('KhÃ´ng tÃ¬m tháº¥y API key. HÃ£y kiá»ƒm tra file .env')
    st.stop()

genai.configure(api_key=api_key)

# HÃ€M Xá»¬ LÃ TÃ€I LIá»†U PDF,DOCX, PPTX, TXT
def get_text(docs):
    text = ''
    try:
        for file in docs:
            if file.name.endswith('.pdf'):
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                    tmp_file.write(file.read())
                    tmp_file_path = tmp_file.name
                pdf_reader = PyPDFLoader(tmp_file_path)
                for page in pdf_reader.load_and_split():
                    text += page.page_content
                os.unlink(tmp_file_path)

            elif file.name.endswith('.docx'):
                with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as tmp_file:
                    tmp_file.write(file.read())
                    tmp_file_path = tmp_file.name
                doc = Document(tmp_file_path)
                for para in doc.paragraphs:
                    text += para.text + '\n'
                os.unlink(tmp_file_path)

            elif file.name.endswith('.pptx'):
                with tempfile.NamedTemporaryFile(delete=False, suffix='.pptx') as tmp_file:
                    tmp_file.write(file.read())
                    tmp_file_path = tmp_file.name
                prs = Presentation(tmp_file_path)
                for slide in prs.slides:
                    for shape in slide.shapes:
                        if hasattr(shape, "text"):
                            text += shape.text + '\n'
                os.unlink(tmp_file_path)

            elif file.name.endswith('.txt'):
                content = file.read().decode('utf-8') 
                text += content + '\n'
                
            elif file.name.endswith(('.png', '.jpg', '.jpeg', '.bmp')):
                with tempfile.NamedTemporaryFile(delete=False, suffix=os.path.splitext(file.name)[1]) as tmp_file:
                    tmp_file.write(file.read())
                    tmp_file_path = tmp_file.name
                try:
                    img = Image.open(tmp_file_path)
                    text += pytesseract.image_to_string(img, lang='vie+eng') + '\n'
                except Exception as e:
                    st.error(f'Lá»—i OCR cho áº£nh {file.name}: {str(e)}')
                finally:
                    os.unlink(tmp_file_path)

            else:
                st.error(f'Äá»‹nh dáº¡ng khÃ´ng há»— trá»£: {file.name}')
    except Exception as e:
        st.error(f'Lá»—i Ä‘á»c tÃ i liá»‡u: {str(e)}')
    return text
# CHIA NHá» VÄ‚N Báº¢N
def get_text_chunk(text):
    try: 
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=1000)
        chunks = text_splitter.split_text(text)
        return chunks
    except Exception as e:
        st.error(f'Lá»—i chia chunk: {str(e)}')
        return []

# Táº O VÃ€ LÆ¯U VECTOR DATABASE
def get_vector_store(text_chunks):
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')
        vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)
        vector_store.save_local('faiss_index')
        st.success('TÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n tÃ­ch vÃ  lÆ°u xong.')
    except Exception as e:
        st.error(f'Lá»—i lÆ°u FAISS index: {str(e)}')

# Táº O CHUá»–I QA Vá»šI PROMPT
def get_conversation_chain():
    prompt_template = """
    Tráº£ lá»i cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t nháº¥t cÃ³ thá»ƒ dá»±a trÃªn ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p. Náº¿u cÃ¢u tráº£ lá»i khÃ´ng cÃ³ trong ngá»¯ cáº£nh Ä‘Æ°á»£c cung cáº¥p, hÃ£y nÃ³i, "CÃ¢u tráº£ lá»i khÃ´ng cÃ³ trong ngá»¯ cáº£nh."
    KhÃ´ng cung cáº¥p thÃ´ng tin sai lá»‡ch.

    Ngá»¯ cáº£nh: {context}
    CÃ¢u há»i: {question}

    Answer:
    """
    try:
        model = ChatGoogleGenerativeAI(model='gemini-2.0-flash', temperature=0.3)
        prompt = PromptTemplate(template=prompt_template, input_variables=['context', 'question'])
        chain = load_qa_chain(model, chain_type='stuff', prompt=prompt)
        return chain
    except Exception as e:
        st.error(f"Lá»—i khá»Ÿi táº¡o chuá»—i QA: {str(e)}")
        return None


# TRáº¢ Lá»œI CÃ‚U Há»I NGÆ¯á»œI DÃ™NG VÃ€ LÆ¯U Lá»ŠCH Sá»¬
def user_input(user_question):
    try:
        embeddings = GoogleGenerativeAIEmbeddings(model='models/embedding-001')
        if not os.path.exists('faiss_index'):
            st.error('KhÃ´ng tÃ¬m tháº¥y FAISS index. HÃ£y táº£i vÃ  phÃ¢n tÃ­ch tÃ i liá»‡u trÆ°á»›c.')
            return None
        new_db = FAISS.load_local('faiss_index', embeddings, allow_dangerous_deserialization=True)
        docs = new_db.similarity_search(user_question)
        chain = get_conversation_chain()
        if not chain:
            return None
        response = chain(
            {"input_documents": docs, "question": user_question},
            return_only_outputs=True
        )
        answer = response["output_text"]

        # LÆ°u lá»‹ch sá»­
        if "chat_history" not in st.session_state:
            st.session_state.chat_history = []
        st.session_state.chat_history.append({"question": user_question, "answer": answer})
        return answer
    except Exception as e:
        st.error(f'Lá»—i xá»­ lÃ½ cÃ¢u há»i: {str(e)}')
        return None


def show_statistics():
    st.markdown("## ğŸ“Š Thá»‘ng kÃª cÃ¢u há»i")

    if "chat_history" not in st.session_state or not st.session_state.chat_history:
        st.info("ChÆ°a cÃ³ dá»¯ liá»‡u Ä‘á»ƒ thá»‘ng kÃª.")
        return

    questions = [chat["question"] for chat in st.session_state.chat_history]
    st.write(f"Tá»•ng sá»‘ cÃ¢u há»i Ä‘Ã£ há»i: **{len(questions)}**")

    #Thá»‘ng kÃª tá»« khÃ³a phá»• biáº¿n
    st.markdown("### ğŸ§  Top 10 tá»« khÃ³a Ä‘Æ°á»£c há»i nhiá»u nháº¥t")

    all_words = " ".join(questions).lower().split()
    #khÃ´ng cho cÃ¡c tá»« lÃªn top 
    stopwords = set(STOPWORDS) | {"cÃ¢u", "há»i", "nÃ o", "gÃ¬", "lÃ ", "cho", "vá»", "cÃ³", "trong", "vÃ ", "lÃ ", "nhÆ°ng", "thÃ¬", "vá»›i", "cho", "tá»«", "Ä‘áº¿n",
    "má»™t", "nhá»¯ng", "cÃ¡c", "cá»§a", "nÃ y", "kia", "Ä‘Ã³", "áº¥y",
    "tÃ´i", "báº¡n", "chÃºng", "mÃ¬nh", "em", "anh", "chá»‹", "há»",
    "Ã´ng", "bÃ ", "khi", "náº¿u", "vÃ¬", "Ä‘á»ƒ", "cÅ©ng", "Ä‘Ã£", "Ä‘ang",
    "sáº½", "pháº£i", "cÃ²n", "hay", "nhÆ°", "nÃªn"}

    words_filtered = [word for word in all_words if word not in stopwords and len(word) > 3]
    common_words = Counter(words_filtered)
    top_words = dict(common_words.most_common(10))

    # Biá»ƒu Ä‘á»“ bar
    plt.figure(figsize=(6, 4))
    sns.barplot(x=list(top_words.values()), y=list(top_words.keys()), palette='viridis')
    plt.title("Top 10 tá»« khÃ³a Ä‘Æ°á»£c dÃ¹ng nhiá»u nháº¥t")
    plt.xlabel("Táº§n suáº¥t")
    plt.ylabel("Tá»« khÃ³a")
    st.pyplot(plt)

# XUáº¤T Lá»ŠCH Sá»¬ Há»I ÄÃP RA FILE WORD
def export_chat_history_to_word(chat_history):
    doc = Document()
    doc.add_heading('Lá»‹ch sá»­ há»i Ä‘Ã¡p ChatBot', level=1)

    for i, chat in enumerate(chat_history, 1):
        doc.add_paragraph(f"CÃ¢u há»i {i}:", style='List Number')
        doc.add_paragraph(chat['question'])
        doc.add_paragraph(f"CÃ¢u tráº£ lá»i {i}:", style='List Number')
        doc.add_paragraph(chat['answer'])
        doc.add_paragraph()

    file_stream = BytesIO()
    doc.save(file_stream)
    file_stream.seek(0)
    return file_stream

# GIAO DIá»†N STREAMLIT
def main():
    st.set_page_config(page_title='ChatBot phÃ¢n tÃ­ch tÃ i liá»‡u ')
    st.title('ChatBot phÃ¢n tÃ­ch tÃ i liá»‡u')

    user_question = st.text_input('ğŸ“Œ Nháº­p cÃ¢u há»i cá»§a báº¡n sau khi phÃ¢n tÃ­ch tÃ i liá»‡u')

    if user_question:
        answer = user_input(user_question)
        if answer:
            st.markdown("### ğŸ’¬ Bot response:")
            st.write(answer)

    # Hiá»ƒn thá»‹ lá»‹ch sá»­ há»i Ä‘Ã¡p
    if "chat_history" in st.session_state and st.session_state.chat_history:
        st.markdown("---")
        st.markdown("## ğŸ•‘ Chat history")
        for i, chat in enumerate(st.session_state.chat_history):
            st.markdown(f"**Q{i+1}:** {chat['question']}")
            st.markdown(f"**A{i+1}:** {chat['answer']}")
        #file word lÆ°u tÃ i liá»‡u Ä‘Ã£ há»i 
        if st.button("ğŸ“„ Print answer"):
            word_file = export_chat_history_to_word(st.session_state.chat_history)
            st.download_button(
                label="ğŸ“¥ Táº£i file Word lá»‹ch sá»­ chat",
                data=word_file,
                file_name="chat_history.docx",
                mime="application/vnd.openxmlformats-officedocument.wordprocessingml.document"
            )
    if st.checkbox("ğŸ“ˆ Hiá»‡n thá»‘ng kÃª cÃ¢u há»i"):
        show_statistics()
        
    # Sidebar Ä‘á»ƒ táº£i file
    with st.sidebar:
        st.title('ğŸ“ Táº£i tÃ i liá»‡u')
        docs = st.file_uploader(
            'Táº£i tÃ i liá»‡u cá»§a báº¡n lÃªn',
            accept_multiple_files=True,
            type=['pdf', 'docx', 'txt', 'pptx','png', 'jpg', 'jpeg', 'bmp']
        )

        if st.button('PhÃ¢n tÃ­ch tÃ i liá»‡u'):
            if not docs:
                st.error('Vui lÃ²ng táº£i tÃ i liá»‡u trÆ°á»›c')
            else:
                with st.spinner('ğŸ” Äang xá»­ lÃ½ tÃ i liá»‡u...'):
                    raw_text = get_text(docs)
                    if raw_text:
                        text_chunks = get_text_chunk(raw_text)
                        if text_chunks:
                            get_vector_store(text_chunks)
                        else:
                            st.error('KhÃ´ng thá»ƒ chia nhá» ná»™i dung Ä‘Æ°á»£c trÃ­ch xuáº¥t')
                    else:
                        st.error('KhÃ´ng thá»ƒ Ä‘á»c Ä‘Æ°á»£c ná»™i dung tÃ i liá»‡u')
    
if __name__ == '__main__':
    main()